{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cthb8O3LCPM1"
   },
   "source": [
    "# SynthID Text: Watermarking for Generated Text\n",
    "\n",
    "This notebook demonstrates how to use the [SynthID Text library][synthid-code]\n",
    "to apply and detect watermarks on generated text. It is divided into three major\n",
    "sections and intended to be run end-to-end.\n",
    "\n",
    "1.  **_Setup_**: Importing the SynthID Text library, choosing your model (either\n",
    "    [Gemma][gemma] or [GPT-2][gpt2]) and device (either CPU or GPU, depending\n",
    "    on your runtime), defining the watermarking configuration, and initializing\n",
    "    some helper functions.\n",
    "1.  **_Applying a watermark_**: Loading your selected model using the\n",
    "    [Hugging Face Transformers][transformers] library, using that model to\n",
    "    generate some watermarked text, and comparing the perplexity of the\n",
    "    watermarked text to that of text generated by the base model.\n",
    "1.  **_Detecting a watermark_**: Training a detector to recognize text generated\n",
    "    with a specific watermarking configuration, and then using that detector to\n",
    "    predict whether a set of examples were generated with that configuration.\n",
    "\n",
    "As the reference implementation for the\n",
    "[SynthID Text paper in _Nature_][synthid-paper], this library and notebook are\n",
    "intended for research review and reproduction only. They should not be used in\n",
    "production systems. For a production-grade implementation, check out the\n",
    "official SynthID logits processor in [Hugging Face Transformers][transformers].\n",
    "\n",
    "[gemma]: https://ai.google.dev/gemma/docs/model_card\n",
    "[gpt2]: https://huggingface.co/openai-community/gpt2\n",
    "[synthid-code]: https://github.com/google-deepmind/synthid-text\n",
    "[synthid-paper]: https://www.nature.com/\n",
    "[transformers]: https://huggingface.co/docs/transformers/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be-I0MNRbyWT"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface-cli login\n",
    "# paste this in console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "aq7hChW8njFo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current huggingface cache dir: /data1/mingjia/cache/huggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 23:34:16.936943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735774456.950699  917094 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735774456.954839  917094 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# @title Install and import the required Python packages\n",
    "#\n",
    "# @markdown Running this cell may require you to restart your session.\n",
    "\n",
    "# ! pip install synthid-text[notebook]\n",
    "\n",
    "from collections.abc import Sequence\n",
    "import enum\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append(\"/home/mingjia/med_watermark/synthid-text/src\")\n",
    "import os\n",
    "os.environ['HF_HOME'] = \"/data1/mingjia/cache/huggingface\"\n",
    "print(f\"Current huggingface cache dir: {os.environ['HF_HOME']}\")\n",
    "\n",
    "\n",
    "import datasets\n",
    "from synthid_text import detector_mean\n",
    "from synthid_text import logits_processing\n",
    "from synthid_text import synthid_mixin\n",
    "from synthid_text import detector_bayesian\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "#  pip install flax, tensorflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "w9a5nANolFS_"
   },
   "outputs": [],
   "source": [
    "# @title Choose your model.\n",
    "#\n",
    "# @markdown This reference implementation is configured to use the Gemma v1.0\n",
    "# @markdown Instruction-Tuned variants in 2B or 7B sizes, or GPT-2.\n",
    "\n",
    "\n",
    "class ModelName(enum.Enum):\n",
    "  GEMMA_7B = 'google/gemma-7b-it'\n",
    "  MISTRAL_7B = '/data2/mingjia/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "model_name = 'google/gemma-7b-it' # '/data2/mingjia/Mistral-7B-Instruct-v0.2' # @param ['gpt2', 'google/gemma-2b-it', 'google/gemma-7b-it']\n",
    "MODEL_NAME = ModelName(model_name)\n",
    "\n",
    "# if MODEL_NAME is not ModelName.GPT2:\n",
    "#   huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelName.GEMMA_7B: '/data2/mingjia/gemma-7b'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import interpreter_login\n",
    "\n",
    "# interpreter_login()\n",
    "# # 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "B_pe-hG6SW6H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Configure your device\n",
    "#\n",
    "# @markdown This notebook loads models from Hugging Face Transformers into the\n",
    "# @markdown PyTorch deep learning runtime. PyTorch supports generation on CPU or\n",
    "# @markdown GPU, but your chosen model will run best on the following hardware,\n",
    "# @markdown some of which may require a\n",
    "# @markdown [Colab Subscription](https://colab.research.google.com/signup).\n",
    "# @markdown\n",
    "# @markdown * Gemma v1.0 2B IT: Use a GPU with 16GB of memory, such as a T4.\n",
    "# @markdown * Gemma v1.0 7B IT: Use a GPU with 32GB of memory, such as an A100.\n",
    "# @markdown * GPT-2: Any runtime will work, though a High-RAM CPU or any GPU\n",
    "# @markdown   will be faster.\n",
    "\n",
    "DEVICE = torch.device('cuda:0') \n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "UOGvCjyVjjQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "immutabledict({'ngram_len': 2, 'keys': [654, 400], 'num_leaves': 2, 'sampling_table_size': 65536, 'sampling_table_seed': 0, 'context_history_size': 1024, 'device': device(type='cuda', index=0)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Example watermarking config\n",
    "#\n",
    "# @markdown SynthID Text produces unique watermarks given a configuration, with\n",
    "# @markdown the most important piece of a configuration being the `keys`: a\n",
    "# @markdown sequence of unique integers.\n",
    "# @markdown\n",
    "# @markdown This reference implementation uses a fixed watermarking\n",
    "# @markdown configuration, which will be displayed when you run this cell.\n",
    "import immutabledict\n",
    "\n",
    "CONFIG = synthid_mixin.DEFAULT_WATERMARKING_CONFIG\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG['ngram_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "79mekKj5UUZR"
   },
   "outputs": [],
   "source": [
    "# @title Initialize the required constants, tokenizer, and logits processor\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_BATCHES = 320\n",
    "OUTPUTS_LEN = 1024\n",
    "TEMPERATURE = 1.0\n",
    "TOP_K = 50\n",
    "TOP_P = 0.9\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME.value)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "logits_processor = logits_processing.SynthIDLogitsProcessor(\n",
    "    **CONFIG, top_k=TOP_K, temperature=TEMPERATURE, num_leaves=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "hndT3YCQUt6D"
   },
   "outputs": [],
   "source": [
    "# @title Utility functions to load models, compute perplexity, and process prompts.\n",
    "def load_model(\n",
    "    model_name: ModelName,\n",
    "    expected_device: torch.device,\n",
    "    enable_watermarking: bool = False,\n",
    ") -> transformers.PreTrainedModel:\n",
    "  if model_name == ModelName.MISTRAL_7B:\n",
    "    model_cls = (\n",
    "        synthid_mixin.SynthIDMistralForCausalLM\n",
    "        if enable_watermarking\n",
    "        # else transformers.MistralForCausalLM\n",
    "        else transformers.AutoModelForCausalLM\n",
    "    )\n",
    "    model = model_cls.from_pretrained(model_name.value, device_map=expected_device, torch_dtype=torch.float16)\n",
    "    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "  else:\n",
    "    model_cls = (\n",
    "        synthid_mixin.SynthIDGemmaForCausalLM\n",
    "        if enable_watermarking\n",
    "        else transformers.GemmaForCausalLM\n",
    "    )\n",
    "    model = model_cls.from_pretrained(model_name.value, device_map=expected_device, torch_dtype=torch.float16)\n",
    "\n",
    "  if str(model.device) != str(expected_device):\n",
    "    raise ValueError('Model device not as expected.')\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def _compute_perplexity(\n",
    "    outputs: torch.LongTensor,\n",
    "    scores: torch.FloatTensor,\n",
    "    eos_token_mask: torch.LongTensor,\n",
    "    watermarked: bool = False,\n",
    ") -> float:\n",
    "  \"\"\"Compute perplexity given the model outputs and the logits.\"\"\"\n",
    "  len_offset = len(scores)\n",
    "  if watermarked:\n",
    "    nll_scores = scores\n",
    "  else:\n",
    "    nll_scores = [\n",
    "        torch.gather(\n",
    "            -torch.log(torch.nn.Softmax(dim=1)(sc)),\n",
    "            1,\n",
    "            outputs[:, -len_offset + idx, None],\n",
    "        )\n",
    "        for idx, sc in enumerate(scores)\n",
    "    ]\n",
    "  nll_sum = torch.nan_to_num(\n",
    "      torch.squeeze(torch.stack(nll_scores, dim=1), dim=2)\n",
    "      * eos_token_mask.long(),\n",
    "      posinf=0,\n",
    "  )\n",
    "  nll_sum = nll_sum.sum(dim=1)\n",
    "  nll_mean = nll_sum / eos_token_mask.sum(dim=1)\n",
    "  return nll_mean.sum(dim=0)\n",
    "\n",
    "\n",
    "def _process_raw_prompt(prompt: Sequence[str]) -> str:\n",
    "  \"\"\"Add chat template to the raw prompt.\"\"\"\n",
    "  if MODEL_NAME == ModelName.GPT2:\n",
    "    return prompt.decode().strip('\"')\n",
    "  else:\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt.decode().strip('\"')}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs9Ih8r4Dyu5"
   },
   "source": [
    "# 2. Applying a watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "JJ28Aajwu9uD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c79741e2f1c433c8d8e4c0f0f35f86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8510bc092d074616ab4b671a5d4839d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4ad6b4ecf5425f8928c84db7444672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ebaa3f77bb4bda86049efc968be61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m example_inputs \u001b[38;5;241m=\u001b[39m example_inputs[:batch_size]\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     17\u001b[0m     example_inputs,\n\u001b[1;32m     18\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_watermarking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     26\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name, expected_device, enable_watermarking)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m   model_cls \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     18\u001b[0m       synthid_mixin\u001b[38;5;241m.\u001b[39mSynthIDGemmaForCausalLM\n\u001b[1;32m     19\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m enable_watermarking\n\u001b[1;32m     20\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m transformers\u001b[38;5;241m.\u001b[39mGemmaForCausalLM\n\u001b[1;32m     21\u001b[0m   )\n\u001b[0;32m---> 22\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mstr\u001b[39m(expected_device):\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel device not as expected.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/transformers/modeling_utils.py:3715\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3714\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3715\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3731\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3732\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3733\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3734\u001b[0m ):\n\u001b[1;32m   3735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/transformers/utils/hub.py:1079\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1381\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/urllib3/response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 934\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    937\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/urllib3/response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title Generate watermarked output\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 1\n",
    "example_inputs = [\n",
    "    'I enjoy walking with my cute dog',\n",
    "    'I am from New York',\n",
    "    'The test was not so very hard after all',\n",
    "    \"I don't think they can score twice in so short a time\",\n",
    "]\n",
    "example_inputs = example_inputs * (int(batch_size / 4) + 1)\n",
    "example_inputs = example_inputs[:batch_size]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    example_inputs,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=True)\n",
    "torch.manual_seed(0)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    min_new_tokens=100,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print('Output:\\n' + 100 * '-')\n",
    "for i, output in enumerate(outputs):\n",
    "  print(tokenizer.decode(output, skip_special_tokens=True))\n",
    "  print(100 * '-')\n",
    "\n",
    "del inputs, outputs, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2eb8c41e82b47369099699deef0ebd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, Honeybun, but its important to note that she is a barker. She alerts me to every little creature or person that passes by. When we walk on the streets, cars whiz by, people and other dogs pass by, and she feels the need to alert me to all of them, loudly. She also barks at birds, squirrels, and even leaves that rustle in the wind. Ive learned to manage her bark\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# @title Generate watermarked output\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 1\n",
    "example_inputs = [\n",
    "    'I enjoy walking with my cute dog',\n",
    "    'I am from New York',\n",
    "    'The test was not so very hard after all',\n",
    "    \"I don't think they can score twice in so short a time\",\n",
    "]\n",
    "example_inputs = example_inputs * (int(batch_size / 4) + 1)\n",
    "example_inputs = example_inputs[:batch_size]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    example_inputs,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=False)\n",
    "torch.manual_seed(0)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    min_new_tokens=100,\n",
    "    max_new_tokens=100,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "print('Output:\\n' + 100 * '-')\n",
    "for i, output in enumerate(outputs):\n",
    "  print(tokenizer.decode(output, skip_special_tokens=True))\n",
    "  print(100 * '-')\n",
    "\n",
    "del inputs, outputs, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6VJm-ZjJ3Q8"
   },
   "source": [
    "## [Optional] Compare perplexity between watermarked and non-watermarked text\n",
    "\n",
    "Sample [eli5 dataset](https://facebookresearch.github.io/ELI5/) outputs from\n",
    "watermarked and non-watermarked models and verify that:\n",
    "\n",
    "* The [perplexity](https://huggingface.co/docs/transformers/en/perplexity) of\n",
    "  watermarked and non-watermarked text is similar.\n",
    "\n",
    "$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DmSUpOwnPvDc"
   },
   "outputs": [],
   "source": [
    "# @title Load Eli5 dataset with HuggingFace datasets.\n",
    "\n",
    "eli5_prompts = datasets.load_dataset(\"Pavithree/eli5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1AkzSvcGS3xk"
   },
   "outputs": [],
   "source": [
    "# @title Non-watermarked output - perplexity\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = load_model(MODEL_NAME, expected_device=DEVICE)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "nonwm_g_values = []\n",
    "nonwm_eos_masks = []\n",
    "nonwm_outputs = []\n",
    "perplexities = []\n",
    "\n",
    "for batch_id in tqdm.tqdm(range(NUM_BATCHES)):\n",
    "  prompts = eli5_prompts['train']['title'][\n",
    "      batch_id * BATCH_SIZE:(batch_id + 1) * BATCH_SIZE]\n",
    "  prompts = [_process_raw_prompt(prompt.encode()) for prompt in prompts]\n",
    "  inputs = tokenizer(\n",
    "      prompts,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "  _, inputs_len = inputs['input_ids'].shape\n",
    "\n",
    "  outputs = model.generate(\n",
    "      **inputs,\n",
    "      do_sample=True,\n",
    "      max_length=inputs_len + OUTPUTS_LEN,\n",
    "      temperature=TEMPERATURE,\n",
    "      top_k=TOP_K,\n",
    "      top_p=TOP_P,\n",
    "      return_dict_in_generate=True,\n",
    "      output_scores=True,\n",
    "  )\n",
    "\n",
    "  scores = outputs.scores\n",
    "  outputs = outputs.sequences\n",
    "  eos_token_mask = logits_processor.compute_eos_token_mask(\n",
    "      input_ids=outputs[:, inputs_len:],\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "\n",
    "  perplexities.append(_compute_perplexity(outputs, scores, eos_token_mask))\n",
    "\n",
    "  g_values = logits_processor.compute_g_values(\n",
    "      input_ids=outputs[:, inputs_len:],\n",
    "  )\n",
    "\n",
    "  nonwm_g_values.append(g_values.cpu())\n",
    "  nonwm_eos_masks.append(eos_token_mask.cpu())\n",
    "  nonwm_outputs.append(outputs.cpu())\n",
    "\n",
    "  del inputs, prompts, eos_token_mask, g_values, outputs\n",
    "\n",
    "del model, nonwm_g_values, nonwm_eos_masks, nonwm_outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4ddNvIow0Ev"
   },
   "outputs": [],
   "source": [
    "final_perplexity = torch.exp(np.sum(perplexities) / (BATCH_SIZE * NUM_BATCHES))\n",
    "print(f\"Perplexity of unwatermarked model: {final_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0sjGynJh9Zz4"
   },
   "outputs": [],
   "source": [
    "# @title Watermarked output - perplexity\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=True)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "wm_outputs = []\n",
    "wm_g_values = []\n",
    "wm_eos_masks = []\n",
    "perplexities = []\n",
    "\n",
    "for batch_id in tqdm.tqdm(range(NUM_BATCHES)):\n",
    "  prompts = eli5_prompts['train']['title'][\n",
    "      batch_id * BATCH_SIZE:(batch_id + 1) * BATCH_SIZE]\n",
    "  prompts = [_process_raw_prompt(prompt.encode()) for prompt in prompts]\n",
    "  inputs = tokenizer(\n",
    "      prompts,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "  _, inputs_len = inputs['input_ids'].shape\n",
    "\n",
    "  outputs = model.generate(\n",
    "      **inputs,\n",
    "      do_sample=True,\n",
    "      max_length=inputs_len + OUTPUTS_LEN,\n",
    "      temperature=TEMPERATURE,\n",
    "      top_k=TOP_K,\n",
    "      top_p=TOP_P,\n",
    "      return_dict_in_generate=True,\n",
    "      output_scores=True,\n",
    "  )\n",
    "  scores = outputs.scores\n",
    "  outputs = outputs.sequences\n",
    "\n",
    "  # Mask to ignore all tokens after the end-of-sequence token.\n",
    "  eos_token_mask = logits_processor.compute_eos_token_mask(\n",
    "      input_ids=outputs[:, inputs_len:],\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "\n",
    "  perplexities.append(_compute_perplexity(outputs, scores, eos_token_mask, watermarked=True))\n",
    "\n",
    "  g_values = logits_processor.compute_g_values(\n",
    "      input_ids=outputs[:, inputs_len:],\n",
    "  )\n",
    "  wm_outputs.append(outputs.cpu())\n",
    "  wm_g_values.append(g_values.cpu())\n",
    "  wm_eos_masks.append(eos_token_mask.cpu())\n",
    "\n",
    "  del outputs, scores, inputs, prompts, eos_token_mask, g_values\n",
    "\n",
    "del model, wm_outputs, wm_g_values, wm_eos_masks\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLaGjGgbNPEK"
   },
   "outputs": [],
   "source": [
    "final_perplexity = torch.exp(\n",
    "    torch.Tensor(np.sum(perplexities)) / (BATCH_SIZE * NUM_BATCHES)\n",
    ")\n",
    "print(f\"Perplexity of watermarked model: {final_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCBDURjUc-6a"
   },
   "source": [
    "# 3. Detecting a watermark\n",
    "\n",
    "To detect the watermark, you have two options:\n",
    "1.   Use the simple **Mean** scoring function. This can be done quickly and requires no training.\n",
    "2.   Use the more powerful **Bayesian** scoring function. This requires training and takes more time.\n",
    "\n",
    "For full explanation of these scoring functions, see the paper and its Supplementary Materials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_key=42\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(hash_key)\n",
    "np.random.seed(hash_key)\n",
    "torch.manual_seed(hash_key)\n",
    "torch.cuda.manual_seed(hash_key)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ya4rVfgDlKGf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9871834ac3af4792885fbc022fcbab5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4415bc5a93694646b905b9d787baba71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Constants\n",
    "\n",
    "NUM_NEGATIVES = 10000\n",
    "POS_BATCH_SIZE = 32\n",
    "NUM_POS_BATCHES = 313\n",
    "NEG_BATCH_SIZE = 32\n",
    "# Truncate outputs to this length for training.\n",
    "POS_TRUNCATION_LENGTH = 200\n",
    "NEG_TRUNCATION_LENGTH = 200\n",
    "# Pad trucated outputs to this length for equal shape across all batches.\n",
    "MAX_PADDED_LENGTH = 1000\n",
    "\n",
    "model_wm = load_model(\n",
    "      MODEL_NAME,\n",
    "      expected_device=DEVICE,\n",
    "      enable_watermarking=True,\n",
    ")\n",
    "\n",
    "model_uwm = load_model(\n",
    "      MODEL_NAME,\n",
    "      expected_device=DEVICE,\n",
    "      enable_watermarking=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "oD1x8ClskqVw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingjia/.conda/envs/llm/lib/python3.11/site-packages/torch/_functorch/vmap.py:479: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::take. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:81.)\n",
      "  batched_outputs = func(*batched_inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n**Answer:**\\n\\nMolar pregnancy is most commonly seen in women who are pregnant for the first time, particularly those with a history of miscarriage or chromosomal abnormalities. Certain genetic factors and hormonal imbalances also increase the risk of molar pregnancy. Additionally, women']\n",
      "['\\n\\n**Answer:** \\n\\nMolar pregnancy risk factors include hormonal imbalance, certain genetic predispositions, history of miscarriage or molar pregnancy, and gestational trophoblastic disease. Some women with underlying medical conditions, such as diabetes or autoimmune disorders, may also']\n"
     ]
    }
   ],
   "source": [
    "# @title Generate model responses and compute g-values\n",
    "from synthid_text.synthid_generate import generate_responses\n",
    "\n",
    "def generate_responses_old(example_inputs, new_tokens, model):\n",
    "  inputs = tokenizer(\n",
    "      example_inputs,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "\n",
    "  # # @title Watermarked output preparation for detector training\n",
    "  # gc.collect()\n",
    "  # torch.cuda.empty_cache()\n",
    "  _, inputs_len = inputs['input_ids'].shape\n",
    "\n",
    "  outputs = model.generate(\n",
    "      **inputs,\n",
    "      do_sample=True,\n",
    "      min_new_tokens=new_tokens,\n",
    "      max_new_tokens=new_tokens,\n",
    "      temperature=TEMPERATURE,\n",
    "      top_k=TOP_K,\n",
    "      top_p=TOP_P,\n",
    "  )\n",
    "\n",
    "  outputs = outputs[:, inputs_len:]\n",
    "  print(outputs)\n",
    "  decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "  print(decoded_outputs[0])\n",
    "  print(decoded_outputs[1])\n",
    "  \n",
    "  # eos mask is computed, skip first ngram_len - 1 tokens\n",
    "  # eos_mask will be of shape [batch_size, output_len]\n",
    "  eos_token_mask = logits_processor.compute_eos_token_mask(\n",
    "      input_ids=outputs,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "  )[:, CONFIG['ngram_len'] - 1 :]\n",
    "\n",
    "  # context repetition mask is computed\n",
    "  context_repetition_mask = logits_processor.compute_context_repetition_mask(\n",
    "      input_ids=outputs,\n",
    "  )\n",
    "  # context repitition mask shape [batch_size, output_len - (ngram_len - 1)]\n",
    "\n",
    "  combined_mask = context_repetition_mask * eos_token_mask\n",
    "\n",
    "  g_values = logits_processor.compute_g_values(\n",
    "      input_ids=outputs,\n",
    "  )\n",
    "  # g values shape [batch_size, output_len - (ngram_len - 1), depth]\n",
    "\n",
    "  return g_values, combined_mask, decoded_outputs\n",
    "\n",
    "\n",
    "example_inputs = [\n",
    "    '<s>[INST] You are a knowledgeable medical professional. Respond briefly and concisely. Who is at risk for molar pregnancy? [/INST]',\n",
    "    # '<s>[INST] You are a knowledgeable medical professional. Respond briefly and concisely. How do you treat scaly skin? [/INST]',\n",
    "    # \"<s>[INST] You are a knowledgeable medical professional. Respond briefly and concisely. What are the three types of dwarfism? [/INST]\",\n",
    "    # \"<s>[INST] You are a knowledgeable medical professional. Respond briefly and concisely. What are 4 symptoms of sleep apnea? [/INST]\",\n",
    "    # 'I am from New York',\n",
    "    # 'The test was not so very hard after all',\n",
    "    # \"I don't think they can score twice in so short a time\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "      example_inputs,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "prefix_len = inputs[\"input_ids\"].shape[1]\n",
    "new_tokens = 50\n",
    "sample = dict(\n",
    "                do_sample=True,\n",
    "                min_new_tokens=new_tokens,\n",
    "                max_new_tokens=new_tokens,\n",
    "                top_k=TOP_K,\n",
    "                top_p=TOP_P,\n",
    "                temperature=TEMPERATURE\n",
    "            )\n",
    "\n",
    "torch.manual_seed(hash_key)\n",
    "uwm_g_values, uwm_mask, output_no_wm = generate_responses(model_uwm, tokenizer, inputs, sample, logits_processor, CONFIG)\n",
    "\n",
    "torch.manual_seed(hash_key)\n",
    "wm_g_values, wm_mask, output_w_wm = generate_responses(model_wm, tokenizer, inputs, sample, logits_processor, CONFIG)\n",
    "\n",
    "decoded_output_no_wm = tokenizer.batch_decode(output_no_wm[:,prefix_len:], skip_special_tokens=True) #  \n",
    "decoded_output_w_wm = tokenizer.batch_decode(output_w_wm[:,prefix_len:], skip_special_tokens=True)   \n",
    "print(decoded_output_no_wm)\n",
    "print(decoded_output_w_wm)\n",
    "\n",
    "\n",
    "# torch.manual_seed(hash_key)\n",
    "# uwm_g_values, uwm_mask, output_no_wm = generate_responses_old(\n",
    "#     example_inputs, new_tokens, model_uwm\n",
    "# )\n",
    "\n",
    "# torch.manual_seed(hash_key)\n",
    "# wm_g_values, wm_mask, output_w_wm = generate_responses_old(\n",
    "#     example_inputs, new_tokens, model_wm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"test.json\", \"w\") as outfile:\n",
    "#     outfile.write(json.dumps(decoded_output_no_wm, indent=4))\n",
    "\n",
    "# decoded_output_no_wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_no_wm[:,prefix_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPzN6_SInzyG"
   },
   "source": [
    "## Option 1: Mean detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "KMU5Ut7Gnng9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores for watermarked responses:  [0.5272109]\n",
      "Mean scores for unwatermarked responses:  [0.49931973]\n",
      "Weighted Mean scores for watermarked responses:  [0.52984023]\n",
      "Weighted Mean scores for unwatermarked responses:  [0.4934467]\n"
     ]
    }
   ],
   "source": [
    "# @title Get Mean detector scores for the generated outputs.\n",
    "\n",
    "# Watermarked responses tend to have higher Mean scores than unwatermarked\n",
    "# responses. To classify responses you can set a score threshold, but this will\n",
    "# depend on the distribution of scores for your use-case and your desired false\n",
    "# positive / false negative rates.\n",
    "\n",
    "wm_mean_scores = detector_mean.mean_score(\n",
    "    wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
    ")\n",
    "uwm_mean_scores = detector_mean.mean_score(\n",
    "    uwm_g_values.cpu().numpy(), uwm_mask.cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Mean scores for watermarked responses: ', wm_mean_scores)\n",
    "print('Mean scores for unwatermarked responses: ', uwm_mean_scores)\n",
    "\n",
    "\n",
    "# You may find that the Weighted Mean scoring function gives better\n",
    "# classification performance than the Mean scoring function (in particular,\n",
    "# higher scores for watermarked responses). See the paper for full details.\n",
    "\n",
    "wm_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
    "    wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
    ")\n",
    "uwm_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
    "    uwm_g_values.cpu().numpy(), uwm_mask.cpu().numpy()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Weighted Mean scores for watermarked responses: ', wm_weighted_mean_scores\n",
    ")\n",
    "print(\n",
    "    'Weighted Mean scores for unwatermarked responses: ',\n",
    "    uwm_weighted_mean_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt_7oe63n-1U"
   },
   "source": [
    "## Option 2: Bayesian detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "4BCwZfgGdH8A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 3/3 [00:03<00:00,  1.28s/it]\n",
      "Downloading readme: 100%|| 78.0/78.0 [00:00<00:00, 683kB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|| 656M/656M [00:10<00:00, 61.0MB/s] \n",
      "Downloading data: 100%|| 17.3M/17.3M [00:00<00:00, 28.0MB/s]\n",
      "Downloading data: 100%|| 30.7M/30.7M [00:00<00:00, 45.6MB/s]\n",
      "Generating train split: 216147 examples [00:01, 135671.53 examples/s]\n",
      "Generating validation split: 3020 examples [00:00, 72785.56 examples/s]\n",
      "Generating test split: 10000 examples [00:00, 167960.28 examples/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 0/313 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     18\u001b[0m     prompts,\n\u001b[1;32m     19\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     22\u001b[0m _, inputs_len \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOUTPUTS_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m wm_outputs\u001b[38;5;241m.\u001b[39mappend(outputs[:, inputs_len:])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m outputs, inputs, prompts\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:2971\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2968\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2969\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 2971\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   2972\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   2973\u001b[0m ):\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title Generate watermarked samples for training Bayesian detector\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=True)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "eli5_prompts = datasets.load_dataset(\"Pavithree/eli5\")\n",
    "\n",
    "wm_outputs = []\n",
    "\n",
    "for batch_id in tqdm.tqdm(range(NUM_POS_BATCHES)):\n",
    "  prompts = eli5_prompts['train']['title'][\n",
    "      batch_id * POS_BATCH_SIZE:(batch_id + 1) * POS_BATCH_SIZE]\n",
    "  prompts = [_process_raw_prompt(prompt.encode()) for prompt in prompts]\n",
    "  inputs = tokenizer(\n",
    "      prompts,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "  _, inputs_len = inputs['input_ids'].shape\n",
    "\n",
    "  outputs = model.generate(\n",
    "      **inputs,\n",
    "      do_sample=True,\n",
    "      max_length=inputs_len + OUTPUTS_LEN,\n",
    "      temperature=TEMPERATURE,\n",
    "      top_k=TOP_K,\n",
    "      top_p=TOP_P,\n",
    "  )\n",
    "\n",
    "  wm_outputs.append(outputs[:, inputs_len:])\n",
    "\n",
    "  del outputs, inputs, prompts\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fsTUs7P3z9j2"
   },
   "outputs": [],
   "source": [
    "# @title Generate unwatermarked samples for training Bayesian detector\n",
    "\n",
    "dataset, info = tfds.load('wikipedia/20230601.en', split='train', with_info=True)\n",
    "\n",
    "dataset = dataset.take(10000)\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "df = tfds.as_dataframe(dataset, info)\n",
    "ds = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "tf.random.set_seed(0)\n",
    "ds = ds.shuffle(buffer_size=10_000)\n",
    "ds = ds.batch(batch_size=1)\n",
    "\n",
    "tokenized_uwm_outputs = []\n",
    "lengths = []\n",
    "batched = []\n",
    "# Pad to this length (on the right) for batching.\n",
    "padded_length = 2500\n",
    "for i, batch in tqdm.tqdm(enumerate(ds)):\n",
    "  responses = [val.decode() for val in batch['text'].numpy()]\n",
    "  inputs = tokenizer(\n",
    "      responses,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "  line = inputs['input_ids'].cpu().numpy()[0].tolist()\n",
    "  if len(line) >= padded_length:\n",
    "    line = line[:padded_length]\n",
    "  else:\n",
    "    line = line + [\n",
    "        tokenizer.eos_token_id for _ in range(padded_length - len(line))\n",
    "    ]\n",
    "  batched.append(torch.tensor(line, dtype=torch.long, device=DEVICE)[None, :])\n",
    "  if len(batched) == NEG_BATCH_SIZE:\n",
    "    tokenized_uwm_outputs.append(torch.cat(batched, dim=0))\n",
    "    batched = []\n",
    "  if i > NUM_NEGATIVES:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UA6iSRKmklTM"
   },
   "outputs": [],
   "source": [
    "# @title Train the Bayesian detector\n",
    "bayesian_detector, test_loss = (\n",
    "    detector_bayesian.BayesianDetector.train_best_detector(\n",
    "        tokenized_wm_outputs=wm_outputs,\n",
    "        tokenized_uwm_outputs=tokenized_uwm_outputs,\n",
    "        logits_processor=logits_processor,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_device=DEVICE,\n",
    "        max_padded_length=MAX_PADDED_LENGTH,\n",
    "        pos_truncation_length=POS_TRUNCATION_LENGTH,\n",
    "        neg_truncation_length=NEG_TRUNCATION_LENGTH,\n",
    "        verbose=True,\n",
    "        learning_rate=3e-3,\n",
    "        n_epochs=100,\n",
    "        l2_weights=np.zeros((1,)),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "wt_xWiSHkvX3"
   },
   "outputs": [],
   "source": [
    "# @title Get Bayesian detector scores for the generated outputs.\n",
    "\n",
    "# Watermarked responses tend to have higher Bayesian scores than unwatermarked\n",
    "# responses. To classify responses you can set a score threshold, but this will\n",
    "# depend on the distribution of scores for your use-case and your desired false\n",
    "# positive / false negative rates. See the paper for full details.\n",
    "\n",
    "wm_bayesian_scores = bayesian_detector.score(\n",
    "    wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
    ")\n",
    "uwm_bayesian_scores = bayesian_detector.score(\n",
    "    uwm_g_values.cpu().numpy(), uwm_mask.cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Bayesian scores for watermarked responses: ', wm_bayesian_scores)\n",
    "print('Bayesian scores for unwatermarked responses: ', uwm_bayesian_scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
