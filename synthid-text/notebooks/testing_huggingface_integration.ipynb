{
  "cells": [
    {
      "metadata": {
        "id": "NoUhNft-ljEw"
      },
      "cell_type": "markdown",
      "source": [
        "# SynthID Text Integration Tests\n",
        "\n",
        "This colab provides integration tests to ensure that the SynthID Text library\n",
        "works correctly with the Hugging Face Transformers library. Run this notebook \n",
        "end-to-end to execute the integration tests on CPU and GPU using a GPT-2 model."
      ]
    },
    {
      "metadata": {
        "id": "TpGoclzNMWyL"
      },
      "cell_type": "markdown",
      "source": [
        "## Test setup"
      ]
    },
    {
      "metadata": {
        "id": "x08GqtnXljEw"
      },
      "cell_type": "code",
      "source": [
        "# @title Install and import the required Python libraries\n",
        "#\n",
        "# @markdown _This may require you to restart your session._\n",
        "\n",
        "! pip install -q synthid-text\n",
        "\n",
        "from collections.abc import Mapping, Sequence\n",
        "from typing import Any\n",
        "\n",
        "from synthid_text import g_value_expectations\n",
        "from synthid_text import logits_processing\n",
        "from synthid_text import synthid_mixin\n",
        "import torch\n",
        "import tqdm\n",
        "import transformers"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d_cU-Cr8ljEw"
      },
      "cell_type": "code",
      "source": [
        "# @title Define integration test logic\n",
        "\n",
        "def initiate_test_model(\n",
        "    *,\n",
        "    ngram_len: int,\n",
        "    keys: Sequence[int],\n",
        "    sampling_table_size: int,\n",
        "    sampling_table_seed: int,\n",
        "    context_history_size: int,\n",
        "    device: torch.device,\n",
        "    vocab_size: int,\n",
        "    pretrained_model_name_or_path: str,\n",
        ") -\u003e 'TestModel':\n",
        "  config = {\n",
        "      'ngram_len': ngram_len,\n",
        "      'keys': keys,\n",
        "      'sampling_table_size': sampling_table_size,\n",
        "      'sampling_table_seed': sampling_table_seed,\n",
        "      'context_history_size': context_history_size,\n",
        "      'device': device,\n",
        "  }\n",
        "\n",
        "\n",
        "  class TestLogitsProcessor(transformers.LogitsProcessor):\n",
        "    \"\"\"Test logits processor used for bias testing later in the colab.\"\"\"\n",
        "\n",
        "    @torch.no_grad\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        scores: torch.FloatTensor,\n",
        "    ) -\u003e torch.FloatTensor:\n",
        "      \"\"\"Create and return uniform scores for bias testing.\"\"\"\n",
        "      return torch.ones_like(scores, device=scores.device)\n",
        "\n",
        "\n",
        "  class TestModelMixin(synthid_mixin.SynthIDSparseTopKMixin):\n",
        "\n",
        "    def _construct_warper_list(\n",
        "        self,\n",
        "        extra_params: Mapping[str, Any],\n",
        "        watermark_config: Mapping[str, Any] = config,\n",
        "    ) -\u003e transformers.LogitsProcessorList:\n",
        "      \"\"\"Instantiate warpers list.\"\"\"\n",
        "      warpers = transformers.LogitsProcessorList()\n",
        "      # Add a logits warper that converts logits to uniform for testing.\n",
        "      warpers.append(TestLogitsProcessor())\n",
        "      extra_params['top_k'] = vocab_size\n",
        "      warpers.append(\n",
        "          logits_processing.SynthIDLogitsProcessor(\n",
        "              **watermark_config, **extra_params\n",
        "          )\n",
        "      )\n",
        "      return warpers\n",
        "\n",
        "\n",
        "  class TestModel(TestModelMixin, transformers.GPT2LMHeadModel):\n",
        "    pass\n",
        "\n",
        "\n",
        "  return TestModel.from_pretrained(\n",
        "      pretrained_model_name_or_path,\n",
        "      device_map=device,\n",
        "  )\n",
        "\n",
        "def test_mean_g_value_matches_theoretical_integrated(\n",
        "    *,\n",
        "    vocab_size: int,\n",
        "    ngram_len: int,\n",
        "    keys: Sequence[int],\n",
        "    device: torch.device,\n",
        "    pretrained_model_name_or_path: str,\n",
        "    batch_size: int,\n",
        "    num_repeats: int,\n",
        "    outputs_len: int,\n",
        "    atol: float,\n",
        "    sampling_table_size: int = 2**16,\n",
        "    sampling_table_seed: int = 0,\n",
        "    context_history_size: int = 1024,\n",
        ") -\u003e tuple[float, float]:\n",
        "  \"\"\"Tests the value of the mean g-value in the sampling loop.\"\"\"\n",
        "  model = initiate_test_model(\n",
        "      ngram_len=ngram_len,\n",
        "      keys=keys,\n",
        "      sampling_table_size=sampling_table_size,\n",
        "      sampling_table_seed=sampling_table_seed,\n",
        "      context_history_size=context_history_size,\n",
        "      device=device,\n",
        "      vocab_size=vocab_size,\n",
        "      pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "  )\n",
        "  # Turn off EOS token stopping.\n",
        "  generation_config = transformers.GenerationConfig.from_model_config(\n",
        "      model.config\n",
        "  )\n",
        "  generation_config.eos_token_id = None\n",
        "  generation_config.stop_strings = None\n",
        "\n",
        "  logits_processor = logits_processing.SynthIDLogitsProcessor(\n",
        "      ngram_len=ngram_len,\n",
        "      keys=keys,\n",
        "      sampling_table_size=sampling_table_size,\n",
        "      sampling_table_seed=sampling_table_seed,\n",
        "      context_history_size=context_history_size,\n",
        "      device=device,\n",
        "      top_k=vocab_size,\n",
        "      temperature=0.7,\n",
        "  )\n",
        "\n",
        "  inputs_len = ngram_len - 1\n",
        "  input_ids = torch.zeros(\n",
        "      (batch_size, inputs_len),\n",
        "      dtype=torch.int64,\n",
        "      device=device,\n",
        "  )\n",
        "  inputs = {\n",
        "      'input_ids': input_ids,\n",
        "      'attention_mask': torch.ones_like(input_ids, device=device),\n",
        "  }\n",
        "\n",
        "  expected_mean_g_value = g_value_expectations.expected_mean_g_value(\n",
        "      vocab_size=vocab_size,\n",
        "  )\n",
        "  mean_g_values_repeats = []\n",
        "\n",
        "  torch.manual_seed(0)\n",
        "  for i in tqdm.tqdm(range(num_repeats)):\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=1.0,\n",
        "        top_k=vocab_size,\n",
        "        max_length=inputs_len + outputs_len,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "    g_values = logits_processor.compute_g_values(\n",
        "        input_ids=outputs[:, inputs_len:],\n",
        "    )\n",
        "    context_repetition_mask = logits_processor.compute_context_repetition_mask(\n",
        "        input_ids=outputs[:, inputs_len:],\n",
        "    ).unsqueeze(dim=2)\n",
        "    mean_g_values = torch.masked.mean(\n",
        "        g_values,\n",
        "        mask=context_repetition_mask,\n",
        "        dim=0,\n",
        "        keepdim=True,\n",
        "        dtype=torch.float64,\n",
        "    )\n",
        "    mean_g_values_repeats.append(mean_g_values)\n",
        "\n",
        "  mean_g_values = torch.concat(mean_g_values_repeats, dim=0).mean(dim=0)\n",
        "  is_close = torch.isclose(\n",
        "      mean_g_values,\n",
        "      torch.tensor(expected_mean_g_value, dtype=torch.float64),\n",
        "      atol=atol,\n",
        "      rtol=0,\n",
        "  )\n",
        "\n",
        "  return mean_g_values, expected_mean_g_value, is_close"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "FY9jlROgljEw"
      },
      "cell_type": "code",
      "source": [
        "# @title Define all test parameters\n",
        "\n",
        "COMMON_CONFIG = dict(\n",
        "    batch_size=400,\n",
        "    outputs_len=30,\n",
        "    keys=[38],\n",
        "    atol=0.01,\n",
        "    pretrained_model_name_or_path=(\n",
        "        'trl-internal-testing/tiny-random-GPT2LMHeadModel'\n",
        "    ),\n",
        ")\n",
        "\n",
        "VOCAB_15_NGRAM_6_CONFIG = dict(\n",
        "    vocab_size=15,\n",
        "    ngram_len=6,\n",
        "    num_repeats=100,\n",
        "    **COMMON_CONFIG\n",
        ")\n",
        "\n",
        "VOCAB_1000_NGRAM_10_CONFIG = dict(\n",
        "    vocab_size=1000,\n",
        "    ngram_len=10,\n",
        "    num_repeats=50,\n",
        "    **COMMON_CONFIG\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "gnS7_O51ljEw"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Invocations"
      ]
    },
    {
      "metadata": {
        "id": "BB3wybmhljEw"
      },
      "cell_type": "code",
      "source": [
        "# @title Run parameterized tests on CPU\n",
        "\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "for test_config in (VOCAB_15_NGRAM_6_CONFIG, VOCAB_1000_NGRAM_10_CONFIG):\n",
        "  result = test_mean_g_value_matches_theoretical_integrated(\n",
        "      device=DEVICE, **test_config\n",
        "  )\n",
        "  print(result)\n",
        "  del result"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GvnooTrgljEw"
      },
      "cell_type": "code",
      "source": [
        "# @title  Run parameterized tests on GPU\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  raise RuntimeError(\n",
        "      'Attempted to run tests on a GPU when no GPU is available.'\n",
        "  )\n",
        "\n",
        "DEVICE = torch.device('cuda:0')\n",
        "\n",
        "for test_config in (VOCAB_15_NGRAM_6_CONFIG, VOCAB_1000_NGRAM_10_CONFIG):\n",
        "  result = test_mean_g_value_matches_theoretical_integrated(\n",
        "      device=DEVICE, **test_config\n",
        "  )\n",
        "  print(result)\n",
        "  del result"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
